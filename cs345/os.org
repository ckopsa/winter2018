#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+TITLE: CS345 : Operating Systems
#+DATE: <2018-01-20 Sat>
#+AUTHOR: Colton Kopsa
#+EMAIL: Aghbac@Aghbac.local
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.3.1 (Org mode 9.1.6)

* DONE Homework 1 [9/9]
  CLOSED: [2018-01-20 Sat 11:41]
** DONE 1. (2) What is multiprogramming and what is it used for?
   CLOSED: [2018-01-08 Mon 11:56]
   Multiprogramming is organizing jobs so that the CPU always has one to execute.

   Multiprogramming is used for increased CPU utilization.
** DONE 2. (2) What are some advantages of multiprocessor systems?
   CLOSED: [2018-01-08 Mon 11:46]
   - Increased throughput - more work done in less time
   - Economy of scale - Because they can share resources the can cost less
   - Increased reliability - properly distributed tasks over several processes
     can fail and allow other processors to pick up the remaining tasks,
     graceful degradation
** DONE 3. (3) How do clustered systems differ from multiprocessor systems?
   CLOSED: [2018-01-08 Mon 11:56]
   Clustered systems differ from the multiprocessor systems in that they are
   composed of two or more individual systems - or nodes - joined together.
*** DONE What is required for two machines belonging to a cluster to cooperate to provide a highly available service?
    CLOSED: [2018-01-08 Mon 11:56]
    Redundancy in the systems.
** DONE 4. (3) Why are caches useful?
   CLOSED: [2018-01-08 Mon 15:04]
   Caches provide quicker access to data by buffering the data from a slower
   storage.
*** DONE What problems do caches cause?
    CLOSED: [2018-01-08 Mon 15:04]
    cache coherency - this occurs when data between caches becomes out-of-sync.
*** DONE If a cache can be made as large as the device for which it is caching (for instance, a cache as large as a disk), why not make it that large and eliminate the device? Provide any assumptions or conditions that your answer is based on.
    CLOSED: [2018-01-08 Mon 15:04]
    The cache can be made as large as the device assuming that the cache
    maintains the same storage properties as the device and also assuming that
    the cache is comparable in cost.
** DONE 5. (3) Indicate which following types of operating systems, Time sharing (T), Real time (R), Handheld (H) match with the following properties:
   CLOSED: [2018-01-10 Wed 11:49]
   Uses CPU scheduling and multiprogramming to provide interactive use of a
   system for many users. - Time Sharing

   An operating system used for a few dedicated end-user applications. - Real Time

   The CPU switches rapidly from one user to another. - Time Sharing

   Reads information from sensors and must respond in a fixed amount of time. -
   Real time

   Often found as part of the control system for a device or system. - Real Time
   
   An operating system for a device with a small amount of memory, small
   display and often, a slow processor. - Handheld
** DONE 6. (3) What is the purpose of interrupts?
   CLOSED: [2018-01-10 Wed 11:34]
   To notify the CPU when a device becomes ready for service. 
*** What are the differences between a trap and an interrupt?
    Interrupts are from hardware and traps are from software.
*** Can traps be generated intentionally by a user program? If so, for what purpose?
    Traps are used to execute system calls from software.
** DONE 7. (2) What are the advantages and disadvantages of using memory-mapped I/O to access device control registers?
   CLOSED: [2018-01-10 Wed 11:19]
   The advantage of memory-mapped I/O is that it is simple to use and is faster
   to write bytes than to issue I/O instructions. The disadvantage is that it
   is vulnerable to accidental modification by means of an incorrect pointer to
   an unintended memory region.
** DONE 8. (4) Consider a hypothetical microprocessor having a 16-bit word size (for example, assume that the program counter and the address registers are 16 bits wide).
   CLOSED: [2018-01-17 Wed 15:00]
*** a. What is the maximum memory, in Bytes, that the processor can access directly if it is connected to a “16-bit wide memory” with a 16-bit data bus? [Hint: Stated another way, every time a read is done from RAM, the RAM delivers 16 bits, or 2 bytes of data.]
    131072 bytes
    131.072 Kbytes
    128 Bytes
*** b. What is the maximum memory, in Kbytes that the processor can access directly if it is connected to an “8-bit wide memory” with a 8-bit wide data bus?
    256 bytes
    .256 Kbytes
    .25 Kbytes
*** c. What architectural features will allow this microprocessor to access a separate “I/O space” (it would have a memory space and I/O space)?
    Special I/O instructions that specify the transfer of a byte or word to an
    I/O port address.
*** d. If separate input and output instructions can each specify which I/O port (usually there is one device for each I/O port) to select using an 8-bit I/O port number, how many I/O ports can the microprocessor support if the data path (data buss) to each I/O port is 8 bits wide? How many I/O ports can the microprocessor support if the data path is 16 bits wide? Explain.
    The format of the instruction might be like:
    | Bits indicating an IO instruction. | Register to read/write from/to | R/W | Port # that is 8 bits |
    - 256 : 2^8 devices
    - 65536 : 2^16 devices
** DONE 9. (2) In virtually all systems that include DMA modules, DMA access to main memory is given higher priority than processor access to main memory. Why?
   CLOSED: [2018-01-10 Wed 11:37]
   The CPU has caches to rely on for data items, and though /cycle stealing/
   can slow down the CPU computation, offloading the data-transfer work to a
   DMA controller generally improves the total system performance.
* DONE Homework 2 [6/6]
  CLOSED: [2018-01-22 Mon 11:35] DEADLINE: <2018-01-22 Mon>
** DONE 1.	(4) List six services provided by an operating system that are designed to make it more convenient for users to use the computer system.
   CLOSED: [2018-01-22 Mon 11:35]
   1. User Interface
   2. Error Detection
   3. File Systems
   4. Program Execution
   5. I/O Operations
   6. Communication
*** DONE Explain why it is not desirous to have the user level provide low-level file-system manipulation services that an operating system typically provides.
    CLOSED: [2018-01-19 Fri 15:14]
    Because the user doesn't have a full understanding of the uses of the
    kernel, they may accidently make an irreversible mistake. Instead, it would
    be better to only allow the user to do safe actions.
** DONE 2.	(2) What are the three general methods that are used for passing parameters to the operating system when a user's process makes a system call?
   CLOSED: [2018-01-19 Fri 14:59]
   1. Pass parameters in registers.
   2. Push parameters to the stack
   3. Store parameters in a block in memory and pass the address as the parameter
** DONE 3.	(3) What is the purpose of the command interpreter?
   CLOSED: [2018-01-19 Fri 14:55]
   It allows users to directly enter commands to be performed by the operating
   system.
*** DONE Why is it usually separate from the kernel?
    CLOSED: [2018-01-19 Fri 14:55]
    It is usually not part of the kernel since the command interpreter is
    subject to changes.
*** DONE Would it be possible for the user to develop a new command interpreter using the system-call interface provided by the operating system?
    CLOSED: [2018-01-19 Fri 14:56]
    Because you can create and manage processes and communicate between things
    from system calls, you could definitely make a new command interpreter.
    
** DONE 4.	(3) What are the two models of interprocess communication?
   CLOSED: [2018-01-19 Fri 14:47]
   Shared Memory and Message Passing
*** DONE What are the strengths and weaknesses of the two approaches?
    CLOSED: [2018-01-19 Fri 14:47]
    Message passing is useful for exchanging smaller amounts of data, because no
    conflicts need be avoided. It is also easier to implement. However, it is
    slow and is for smaller data exchanges.

    Shared Memory has the speed and size of main memory; however, it has
    problems relating to security and synchronization.
** DONE 5.	(4.5) What are the advantages of the microkernel approach to system design?
   CLOSED: [2018-01-19 Fri 14:49]
   It is smaller and easier to manage.
*** DONE How do user programs and system services interact in a microkernel architecture?
    CLOSED: [2018-01-19 Fri 14:48]
    Message passing
*** DONE What are the disadvantages of using the microkernel approach?
    CLOSED: [2018-01-19 Fri 14:48]
    The performance of mircokernels can suffer due to increased system-function
    overhead.
** DONE 6.	(2.5) What is the purpose of system calls, and how do system calls relate to the OS and to the concept of dual-mode (kernel mode and user mode) operation?
   CLOSED: [2018-01-22 Mon 11:35]
   System calls provide the means for a user program to ask the operating system
   to perform tasks reserved for the operating system on the user program’s
   behalf. When a system call is made this utilizes the dual mode of the
   operating system by switching from user mode to kernel mode.
   
* DONE Reading Chapter 3
  CLOSED: [2018-01-22 Mon 11:24] DEADLINE: <2018-01-22 Mon>
  Reading Processes: concept, scheduling, operations on (3.1 – 3.3)
** Notes
   - ___-bound process - different processes can be bottle-necked by I/O or CPU
   - job queue - linked list of all processes in the system
   - process control
   - scheduler - determines which job to execute next
   - device queue - list of processes waiting for a particular I/O device
   - context switch - save off the caches in the CPU to memory to switch to
     another process
   - cascading termination - when a process is terminated, all process children
     are also terminated
* DONE Lab 1-T2
  CLOSED: [2018-01-22 Mon 21:10] DEADLINE: <2018-01-23 Tue>
* DONE GP1
  CLOSED: [2018-01-22 Mon 21:10] DEADLINE: <2018-01-23 Tue>
* DONE Reading Chapter 3 cont.
  CLOSED: [2018-01-24 Wed 10:38] DEADLINE: <2018-01-24 Wed>
  :LOGBOOK:
  CLOCK: [2018-01-24 Wed 10:05]--[2018-01-24 Wed 10:37] =>  0:32
  :END:
  IPC, Client-Server (3.4, 3.5.3, 1st paragraph in 3.6.1, three paragraphs in
  3.6.2, 3.6.3 – 3.7)
** Notes
* DONE Quiz #1
  CLOSED: [2018-01-26 Fri 09:36] DEADLINE: <2018-01-25 Thu>
* DONE Reading Chapter 6
  CLOSED: [2018-01-26 Fri 11:33] DEADLINE: <2018-01-26 Fri>
  CPU scheduling (6.1 – 6.3.3)
** Notes
   - Preemptive scheduling :
     - When a process switches from the running state to the waiting state.
     - When a process switches from the running state to the ready state.
     - When a process switches from the waiting state to the ready state.
     - When a process terminates.
* DONE Lab 2
  CLOSED: [2018-01-27 Sat 22:34] DEADLINE: <2018-01-27 Sat>
** Conclusions
   - I was successful on the first part of the lab
   - I was successful on the second part of the lab
   - One difficulty I encountered was forgetting to add the '#define' to the
     '/usr/include/asm/unistd.h' file. This required that I reboot an extra time
     and trace my steps to make sure everything was placed properly.
   - I learned that modifications to the kernel are somewhat simple. I also
     learned that the kernel could be compiled and installed on a machine while
     it was running, but just requires a reboot for things changes to be made to
     the running kernel.
   - I think it would be easier to make a custom distribution on an ISO for this
     lab that way any desired virtual machine software could be used, as opposed
     to having to use VMware.
** Hello World Screenshot
   [[./hello_world.png]]
** Write File Screenshot
   [[./write_file.png]]
* DONE Homework 3 [8/8]
  CLOSED: [2018-01-26 Fri 09:36]
** DONE (2) What user actions or OS events/actions initiate the creation of a process on a system? [Note that question is not asking for what happens during the creation of a process.]
   CLOSED: [2018-01-22 Mon 16:01]
   Double-clicking the executable and running it from the command line
** DONE (4.5) Describe the differences between short-term, medium-term, and long-term scheduling.
   CLOSED: [2018-01-22 Mon 16:16]
   The main difference between schedulers is how often they are run:
   - Long-term : selects processes from the mass-storage device pool and loads
     them into memory for execution. Invoked infrequently
   - Short-term : Selects from among the processes that are ready to execute and
     allocates the CPU to one of them.
   - Medium-term : can be added if degree of multiple programming needs to
     decrease. Remove process from memory, store on disk, bring back in from
     disk to continue execution: swapping.
** DONE (3) Describe the actions taken by a kernel to context-switch between processes.
   CLOSED: [2018-01-26 Fri 09:14]
   When a context switch occurs, the kernel saves the context of the old process
   in its PCB and loads the saved context of the new process scheduled to run
   
   1. In response to a clock interrupt, the OS saves the PC and user stack
      pointer of the currently executing process, and transfers control to the
      kernel clock interrupt handler
   2. The clock interrupt handler saves the rest of the registers, as well as
      other machine state, such as the state of the floating point registers, in
      the process PCB.
   3. The OS invokes the scheduler to determine the next process to execute,
   4. The OS then retrieves the state of the next process from its PCB, and
      restores the registers. This restore operation takes the processor back to
      the state in which this process was previously interrupted, executing in
      user code with user mode privileges.

** DONE (3) Log in to a Linux lab node and do the following commands:
   CLOSED: [2018-01-26 Fri 09:36]
   - ps axjf
     - PPID
     - PID
     - PGID
     - SID
     - TTY
     - TPGID
     - STAT
     - UID
     - TIME
     - COMMAND 
   - ps –efH
     - no output :(
*** DONE What do these commands show?
    CLOSED: [2018-01-26 Fri 09:36]

** DONE (2) What is swapping and what is its purpose?
   CLOSED: [2018-01-26 Fri 09:18]
   The key idea behind a medium-term scheduler is that sometimes it can be
   advantageous to remove a process from memory (and from active contention for
   the CPU ) and thus reduce the degree of multiprogramming. Later, the process
   can be reintroduced into memory, and its execution can be continued where it
   left off. This scheme is called swapping. The process is swapped out, and is
   later swapped in, by the medium-term scheduler. Swapping may be necessary to
   improve the process mix or because a change in memory requirements has
   overcommitted available memory, requiring memory to be freed up.

** DONE (2) In a number of early computers, an interrupt caused the register values to be stored in fixed locations associated with the given interrupt signal. Under what circumstances is this a practical technique? Explain why it is inconvenient in general.
   CLOSED: [2018-01-26 Fri 09:29]
   This technique is based on the assumption that an interrupted process /A/
   will continue to run after the response to an interrupt. But, in general, an
   interrupt may cause that basic monitor to preempt a process /A/ in favor of
   another process /B/. It is now necessary to copy the execution state of
   process /A/ from the location associated with the interrupt to the process
   description associated with /A/. The machine might as well have stored them
   there in the first place.

   This technique is based on the assumption that an interrupted process /A/ will
   continue to run after the response to an interrupt. But, in general, an
   interrupt may cause the basic monitor to preempt a process /A/ in favor of
   another process /B/. It is now necessary to copy the execution state of process
   /A/ from the location associated with the interrupt to the process description
   associated with /A/. The machine might as well have stored them there in the
   first place.
** DONE (1.5) When a process creates a new process using the fork() operation, which of the following state is shared between the parent process and the child process?
   CLOSED: [2018-01-24 Wed 10:31]
   - Stack
   - Heap
   - Shared memory segments
     The child inherits open files from its parent and pipes.
** DONE (2) Discuss at least two major complications that multiprogramming adds to an operating system.
   CLOSED: [2018-01-26 Fri 09:24]
   1. Multiprogramming requires complex scheduling in order to make sure all
      active processes get time on the processor.
   2. Multiprogramming requires each process to have its own memory, and which
      process has which block of memory must be managed.
   
* DONE Homework 4 [10/10]
  CLOSED: [2018-01-31 Wed 14:36] DEADLINE: <2018-02-02 Fri>
  #+BEGIN_SRC sh
    open hw4-ch6_osc_wsosim.doc
  #+END_SRC
  
  #+RESULTS:

** DONE What is usually the critical performance requirement in an interactive operating system?
   CLOSED: [2018-01-26 Fri 11:21]
   Adequate response time
** DONE What is the difference between turnaround time and response time?
   CLOSED: [2018-01-26 Fri 11:15]
   - Turnaround time : From the point of view of a particular process, the
     important criterion is how long it takes to execute that process. */The
     interval from the time of submission of a process to the time of completion
     is the turnaround time/*. Turnaround time is the sum of the periods spent
     waiting to get into memory, waiting in the ready queue, executing on the
     CPU, and doing I/O.
   - Response time : In an interactive system, turnaround time may not be the
     best criterion. Often, a process can produce some output fairly early and
     can continue computing new results while previous results are being output
     to the user. Thus, another measure is */the time from the submission of a
     request until the first response is produced/*. This measure, called
     response time, is the time it takes to start responding, not the time it
     takes to output the response. The turnaround time is generally limited by
     the speed of the output device.
** DONE What is the difference between preemptive and non-preemptive scheduling?
   CLOSED: [2018-01-26 Fri 11:18]
   - non-preemptive scheduling : there is no choice in terms of scheduling.
   - preemptive scheduling : there is choice in terms of scheduling.
** DONE Consider the exponential average formula used to predict the length of the next CPU burst. What are the implications of assigning the following values to the parameters used by the algorithm?
   CLOSED: [2018-01-26 Fri 11:32]
   a. α = 0 and τ_{0} = 100 milliseconds
   τ_{1} = 0t_0 + (1 - 0) 100
   τ_{1} = 0t_0 + 100 = 100

   b. α = 0.99 and τ_{0} = 10 milliseconds
   τ_{1} = .99t_0 + (1 - .99) 100
   τ_{1} = .99t_0 + .01 * 100
   τ_{1} = .99t_0 + 1.0

** DONE Which of the following scheduling algorithms could result in starvation?
   CLOSED: [2018-01-26 Fri 11:23]
   #+BEGIN_QUOTE
   First-come, first-served 

   Shortest job first 

   *Round Robin Priority*
   #+END_QUOTE

** DONE Consider the following set of processes:
   CLOSED: [2018-01-29 Mon 14:57]
   
   | Process Name | Arrive Time | Service Time |
   |----------------+---------------+----------------|
   | A              |             0 |              3 |
   | B              |             1 |              5 |
   | C              |             3 |              2 |
   | D              |             9 |              5 |
   | E              |            12 |              5 |

   #+BEGIN_QUOTE
   In the second table, darken squares to indicate the order in which processes
   execute. In the first table, fill in the statistics requested. T_{r}/T_{s} =
   Turnaround Time / Service Time. T_{r}/T_{s} is another metric against which
   scheduling algorithms are evaluated. Assume that arriving jobs enter the
   system 'just before' the stated arrival time so that they are in the system
   and are seen by the scheduler at the stated arrival time.
   #+END_QUOTE

   |               |                         |     |   |   |   |    | Mean |
   |---------------+-------------------------+-----+---+---+---+----+------|
   |               | Process                 |   A | B | C | D |  E |      |
   |               | Arrival Time            |   0 | 1 | 3 | 9 | 12 |      |
   |               | Service Time (T_{s})    |   3 | 5 | 2 | 5 |  5 |      |
   | FCFS          | Finish Time             |   3 |   |   |   |    |      |
   |               | Turnaround Time (T_{r}) | 3.0 |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |
   | RR q=1        | Finish Time             |     |   |   |   |    |      |
   |               | Turnaround Time (T_{r}) |     |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |
   | RR q=4        | Finish Time             |     |   |   |   |    |      |
   |               | Turnaround Time (T_{r}) |     |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |
   | SJF           | Finish Time             |     |   |   |   |    |      |
   | Nonpreemptive | Turnaround Time (T_{r}) |     |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |
   | SRT           | Finish Time             |     |   |   |   |    |      |
   |               | Turnaround Time (T_{r}) |     |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |
   | FB q=1        | Finish Time             |     |   |   |   |    |      |
   |               | Turnaround Time (T_{r}) |     |   |   |   |    |      |
   |               | T_{r}/T_{s}             |     |   |   |   |    |      |

   The FB algorithm is a multilevel feedback-queue scheduling algorithm with a
   large number of queues. A process moves to the next lower queue after
   receiving a single time quantum of service even if no other process is in the
   system.\\
   Note: 0, 5, 10, and all other time values are at one point in time. The
   vertical line under the 0 (zero) is where time starts. A process entering the
   system at time 0 is in the ready queue at time 0.

   This following question use the [[http://www.training.com.br/sosim/indexen.htm][SOsim: Simulator for Operating Systems
   Education]] to explore topics and issues dealing with processes and CPU
   scheduling.

   To get started, download and unzip the SOsim program
   ([[http://www.training.com.br/sosim/indexen.htm]] or from Blackboard). Run SOsim
   and explore the various menus and buttons to become familiar with the
   simulator and the statistics it provides.

   Use SOsim to answer the questions below. You might wish to get together with
   other students in the class, after you have individually worked on the
   questions, to discuss what you have learned and to determine your answers.

   Tips for using SOsim: 

   - SOsim doesn't run well on some new PCs with faster processors and lots of
     memory

   - The log window provides some useful information.

   - Some of the statistics in SOsim are of questionable quality or are not what
     you might think they should be.

   - Stopping the simulator, creating processes, and then letting the simulator
     run for a certain amount of time before stopping it again, might prove
     helpful.

   - You may need to exit and restart the simulator before each simulation run.
     /This is especially true when changing options./

   - If priorities are different, your results will vary depending on whether you
     start the IO or CPU job first.

** DONE No Priority Scheduling
   CLOSED: [2018-01-31 Wed 14:07]

   There are a couple of scheduling priority options that are accessed with the
   "Options" menu of the "Processor Manager" window; make sure they are both
   turned off and that you restart SOsim after turning them off. With the options
   off, create two processes, one CPU-bound and the other IO-bound (IO\_1) at
   priority 0, and simulate round-robin scheduling without priority. Run with
   some different settings for "IO wait time" and "Timeslice." What settings
   create the largest waits for processes in the ready queue? Describe what
   settings you used and what results were obtained. Take a look at the various
   windows available under the "Windows" menu.
   
   Minimize IO wait time and maximize time slice, minimize clock

** DONE Starvation
   CLOSED: [2018-01-31 Wed 14:25]

   Using only two processes, simulate starvation. Describe what you did and why
   it produced starvation. What criteria might be utilized to define process
   priority to help prevent starvation?
   
   One process has maximum priority, and the other doesn't. A possible solution
   to starvation is to use a scheduling algorithm with priority queue that also
   uses the aging technique. Aging is a technique of gradually increasing the
   priority of processes that wait in the system for a long time.

** DONE Static Priority Scheduling
   CLOSED: [2018-01-31 Wed 14:30]

   Enable priority preemptive scheduling (dynamic priority scheduling should be
   off; stop and restart SOsim) and create a CPU-bound process at priority level
   three and an IO-bound (IO\_1) process with a priority level of four. What is
   happening? How does this situation compare with that done in item seven? How
   does changing the IO wait time and timeslice affect the simulation?
   
   The CPU process will give way to the IO process and at the end of it's
   timeslice, because it has priority.

** DONE Dynamic Priority Scheduling
   CLOSED: [2018-01-31 Wed 14:35]

   Create three or more processes and run a simulation using dynamic priority
   scheduling (turn off priority preemptive scheduling and restart). What does
   this do? What is the difference between process types IO\_1, IO\_2 and IO\_3?
   Is it possible to starve a process when using dynamic priority scheduling with
   SOsim? [Hint: it helps to start all the processes with the same priority and
   have a CPU bound process in the mix.]
   
   The different inputs have different set priorities, or IO1 is priority 1
   input to 3 and it looks like if enough higher priority IOs exist then the CPU
   bound process is starved.
* DONE Weekly Reading [3/3]
  CLOSED: [2018-02-01 Thu 15:30]
** DONE CPU scheduling (6.3.4 – 6.3.6, 6.5; Lab 3 handout)
   CLOSED: [2018-01-29 Mon 16:19]
   :LOGBOOK:
   CLOCK: [2018-01-29 Mon 09:00]--[2018-01-29 Mon 10:00] =>  1:00
   :END:
** DONE CPU scheduling (6.7 – 6.9)
   CLOSED: [2018-01-31 Wed 11:42]
** DONE Threads (4.1 – 4.4)
   CLOSED: [2018-02-01 Thu 15:30]
* DONE Quiz #2
  CLOSED: [2018-02-01 Thu 15:30] DEADLINE: <2018-02-03 Sat>
  [[https://byui.brightspace.com/d2l/common/dialogs/quickLink/quickLink.d2l?ou=377202&type=schedule&rcode=byui_production-1718001][Quiz #2]] 
* DONE Grade Homework 4
  CLOSED: [2018-02-05 Mon 20:39] SCHEDULED: <2018-02-05 Mon> DEADLINE: <2018-02-05 Mon>
* DONE Lab 3
  CLOSED: [2018-02-08 Thu 08:30] SCHEDULED: <2018-02-06 Tue> DEADLINE: <2018-02-07 Wed>
  [[https://byui.brightspace.com/d2l/common/dialogs/quickLink/quickLink.d2l?ou=377202&type=schedule&rcode=byui_production-1723972][Assignment]]
* DONE Homework 5 [8/8]
  CLOSED: [2018-02-07 Wed 14:52] DEADLINE: <2018-02-07 Wed>
** DONE (3) What code is responsible for doing context switches between the user-level threads in a process? What is this code required to do?
   CLOSED: [2018-02-07 Wed 14:51]
   Operating system, save state and restore state
** DONE (3) Explain how a multithreaded application may provide better performance than a single-threaded application even on a single-processor system.
   CLOSED: [2018-02-05 Mon 16:13]
   When a kernel thread suffers a page fault, another kernel thread can be
   switched in to use the interleaving time in a useful manner. A
   single-threaded process, on the other hand, will not be capable of performing
   useful work when a page fault takes place. Therefore, in scenarios where a
   program might suffer from frequent page faults or has to wait for other
   system events, a multi-threaded solution would perform better even on a
   single-processor system.
** DONE (2) Which of the following components of program state are shared across threads in a multithreaded process? (There may be more than one.)
   CLOSED: [2018-02-05 Mon 16:08]

   A. Register values
   -B. Heap Memory-
   -C. Global variables-
   D. Stack memory

** DONE (3) Discuss three advantages of user-level threads (ULTs) over kernel-level threads (KLTs).
   CLOSED: [2018-02-05 Mon 16:00]
   - Thread scheduling does not involved the OS, thus no system calls with the associated user/kernel mode switching is done.
   - Scheduling may be very application specific.
   - Faster to create, manage, and destroy.
   - May run on any OS

** DONE (2) Discuss two disadvantages of user-level threads (ULTs) over kernel-level threads (KLTs).
   CLOSED: [2018-02-05 Mon 16:04]
   - Blocks for I/O could block the entire process along with all user-level threads.
   - Can't take advantage of a multiprocessor.
** DONE (1) What are the three main thread libraries in use today?
   CLOSED: [2018-02-05 Mon 15:49]
   - Pthreads
   - Java
   - Windows API
** DONE (3) List three threading issues. Briefly explain each of the issues (not how the issue might be solved, just what is the issue).
   CLOSED: [2018-02-07 Wed 14:35]
   - Thread-Local & Thread-Specific - each thread may need a copy of the data
   - Thread canceling - terminating a thread before it has completed.
   - fork() & exec() System Calls - If one thread calls fork, does the new
     process duplicated all the threads? or is it single threaded?
** DONE (3) A Solaris thread which is currently running on a CPU may voluntarily give up the CPU to another thread. Which of the following statements is correct:
   CLOSED: [2018-02-07 Wed 14:38]

   1. A Solaris user-level thread (ULT) may yield to another thread of the same priority.

   *
   2. A Solaris user-level thread (ULT) may yield to another thread of the same priority or higher priority.
   *

   3. A Solaris user-level thread (ULT) may yield to another thread of the same priority, a lower priority, or higher priority.

   Assume that anytime the priority of a thread in the system might change, the user-level thread scheduler is called to determine which process gets the processor. [Help: you might look at the sched_yield (which is for kernel threads, but might still be helpful) by doing: man 2 sched_yield.]

   Explain why you chose the one you did.
   
   Because yielding would allow the next in line to go, but not necessarily the
   next in a lower priority line.

* DONE Weekly Reading [3/3]
  CLOSED: [2018-02-10 Sat 15:10]
** DONE Threads (4.5 – 4.8, 6.4)
   CLOSED: [2018-02-07 Wed 14:38] SCHEDULED: <2018-02-05 Mon> DEADLINE: <2018-02-05 Mon>
** DONE Process Synchronization (5.1 – 5.5)
   CLOSED: [2018-02-07 Wed 14:43] SCHEDULED: <2018-02-06 Tue> DEADLINE: <2018-02-07 Wed>
** DONE Process Synchronization (5.6 – 5.7)
   CLOSED: [2018-02-10 Sat 15:09] DEADLINE: <2018-02-09 Fri>
* DONE Homework 6 [7/7]
  CLOSED: [2018-02-14 Wed 11:57] SCHEDULED: <2018-02-13 Tue> DEADLINE: <2018-02-14 Wed>
** DONE (2) What is the meaning of the term busy waiting?
   CLOSED: [2018-02-12 Mon 15:29]
   Using up CPU cycles to wait for something.
*** What else might be done to wait for a condition to change other than busy wait?
    Blocking 
** DONE (4) What are the requirements that a solution to the critical-section problem must satisfy? Explain each one briefly.
   CLOSED: [2018-02-12 Mon 15:36]

   1. Mutual exclusion. If process P i is executing in its critical section,
      then no other processes can be executing in their critical sections.

   2. Progress. If no process is executing in its critical section and some
      processes wish to enter their critical sections, then only those processes
      that are not executing in their remainder sections can participate in
      deciding which will enter its critical section next, and this selection
      cannot be postponed indefinitely.

   3. Bounded waiting. There exists a bound, or limit, on the number of times
      that other processes are allowed to enter their critical sections after a
      process has made a request to enter its critical section and before that
      request is granted.

** DONE (2) What is the difference between binary and counting semaphores?
   CLOSED: [2018-02-12 Mon 15:41]
   The counting semaphore can range over an unrestricted domain, but the binary
   semaphore is can only range between 0 and 1.
   
** DONE (2) Other Question
   CLOSED: [2018-02-14 Wed 15:14]
   #+BEGIN_SRC C++

     const int n=50;
     int tally; //global variable
     void total ( )
     {
       int count;
       for (count = 1; count <= n; count++)
         {
           tally++;
         }
     }

     void main ( )
     {
       tally = 0;
       parbegin (total ( ), total ( ) ) ; //Start two threads in parallel running total( )
       // and wait for them to complete
       write (tally) ;
     }
   #+END_SRC

   Determine the proper lower bound and upper bound on the final value of the
   shared variable tally output by this concurrent program running on a
   uni-processor. Assume a thread can execute at any relative speed and that a
   value can only be incremented after it has been loaded into a register by a
   separate machine instruction. Assume also that a load is done before the
   increment and a store is done after the increment.
   

   [For most of the credit on this problem, show that you put some thought in to it.]

   Hint: The assembly language instruction stream might be:

   #+BEGIN_EXAMPLE
     load mem -> reg1 
     incr reg1
     store reg1 -> mem
   #+END_EXAMPLE

   Suppose that an arbitrary number of these threads are permitted to execute in
   parallel under the assumptions of part (a). What effect will this
   modification have on the range of final values of tally?

   a. 2 \geq x \geq 50
   b. n \geq x \geq 50
** DONE (2) Explain why implementing synchronization primitives by disabling interrupts is not appropriate in a single-processor system if the synchronization primitives are to be used in user-level programs. [i.e. User programs disable interrupts when executing synchronization primitives.]
   CLOSED: [2018-02-14 Wed 11:55]
   If a user-level program is given the ability to disable interrupts, then it
   can disable the timer interrupt and prevent context switching from taking
   place, thereby allowing it to use the processor without letting other
   processes to execute.
** DONE (2) Servers can be designed to limit the number of open connections. For example, a server may wish to have only N socket connections at any point in time. As soon as N connections are made, the server will not accept another incoming connection until an existing connection is released. Explain how semaphores can be used by a server to limit the number of concurrent connections.
   CLOSED: [2018-02-14 Wed 11:55]
   A semaphore is initialized to the number of allowable open socket
   connections. When a connection is accepted, the acquire() method is called;
   when a connection is released, the release() method is called. If the system
   reaches the number of allowable socket connections, subsequent calls to
   acquire() will block until an existing connection is terminated and the
   release method is invoked.
** DONE (2) How does the signal() operation associated with monitors differ from the corresponding operation defined for semaphores?
   CLOSED: [2018-02-14 Wed 14:19]
   Signal on a monitor does nothing if there is no one waiting; however, with
   the semaphore we still increment the counter.
* DONE Feedback on Lab 3 - T1
  CLOSED: [2018-02-12 Mon 16:37] SCHEDULED: <2018-02-12 Mon> DEADLINE: <2018-02-12 Mon>
  
* DONE Lab 3 - T2 Submission
  CLOSED: [2018-02-13 Tue 08:41] SCHEDULED: <2018-02-13 Tue> DEADLINE: <2018-02-13 Tue>
* DONE Lab 3 - Grade Peers
  CLOSED: [2018-02-13 Tue 08:41] SCHEDULED: <2018-02-13 Tue> DEADLINE: <2018-02-13 Tue>
* DONE Weekly Reading [3/3]
  CLOSED: [2018-02-20 Tue 19:24]
** DONE Process Synchronization (5.8 – 5.9)
   CLOSED: [2018-02-12 Mon 15:27] SCHEDULED: <2018-02-12 Mon> DEADLINE: <2018-02-12 Mon>
** DONE Alternative Approaches (5.10, Lab 5 handout,[[http://arstechnica.com/gadgets/2011/08/ibms-new-transactional-memory-make-or-break-time-for-multithreaded-revolution/][ IBM's New Transactional Memory]] 
   CLOSED: [2018-02-14 Wed 12:11] SCHEDULED: <2018-02-13 Tue> DEADLINE: <2018-02-14 Wed>
** DONE Deadlocks (5.11, 5.12)
   CLOSED: [2018-02-16 Fri 14:19] SCHEDULED: <2018-02-15 Thu> DEADLINE: <2018-02-16 Fri>
* DONE Homework 7 [5/5]
  CLOSED: [2018-02-20 Tue 19:41] SCHEDULED: <2018-02-20 Tue> DEADLINE: <2018-02-21 Wed>
** DONE (2) What are the four conditions that create deadlock? Briefly explain each one.
   CLOSED: [2018-02-16 Fri 14:22]
   - Mutual exclusion. At least one resource must be held in a nonsharable mode;
     that is, only one process at a time can use the resource. If another
     process requests that resource, the requesting process must be delayed
     until the resource has been released.
   - Hold and wait. A process must be holding at least one resource and waiting
     to acquire additional resources that are currently being held by other
     processes.
   - No preemption. Resources cannot be preempted; that is, a resource can be
     released only voluntarily by the process holding it, after that process has
     completed its task.
   - Circular wait. A set {P 0 , P 1 , ..., P n } of waiting processes must
     exist such that P 0 is waiting for a resource held by P 1 , P 1 is waiting
     for a resource held by P 2 , ..., P n−1 is waiting for a resource held by P
     n , and P n is waiting for a resource held by P 0 .
** DONE (2) What are two ways to preempt resources in a system to prevent deadlocks by eliminating the “no preemption” condition necessary for deadlocks.
   CLOSED: [2018-02-16 Fri 14:27]
   
   - Deadlock Avoidance
   - Deadlock Prevention
** DONE (3) [[file:3.jpeg][file:~/classes/cs345/3.jpeg]] 
   CLOSED: [2018-02-20 Tue 19:38]
   A spooling system (figure above) consists of an input process I, a processing
   process P, and an output process O connected by two buffers. The processes
   exchange data in blocks of equal size. These blocks are buffered on a disk
   using a floating boundary between the input and the output buffers depending
   on the speed of the processes. The communication primitives used ensure that
   the following resource constraint is satisfied:

   i + o < max

   where:
   - max = maximum number of blocks on disk
   - i = number of blocks used on the disk for the input buffer
   - o = number of blocks used on the disk for the output buffer

   The following is known about the processes:

   - As long as the environment supplies data, process I will eventually input
     it to the input buffer on the disk (provided disk space becomes available).
   - As long as there is an item(s) in the input buffer on the disk, process P
     will eventually consume it and output a finite amount of data to the output
     buffer on the disk for each block input (provided disk space becomes
     available).
   - As long as there is an item in the output buffer, process O will eventually
     consume it.

   Show that this system can be deadlocked.
   
   Stuff about how it's \geq instead of \gt and how that can cause a problem
** DONE (2) Suggest an additional resource constraint that will prevent the deadlock in problem 3; but still permit the boundary between input and output buffers to vary in accordance with the present needs of the processes.
   CLOSED: [2018-02-20 Tue 19:41]
   Use \gt instead
** DONE (3) Three processes share four instances of an identical resource. The resource instances can only be reserved and released one at a time. Each process needs a maximum of two units. Show that a deadlock cannot occur.
   CLOSED: [2018-02-20 Tue 19:40]
   If the system is deadlocked, it implies that each process is holding one
   resource and is waiting for one more. Since there are 3 processes and 4
   resources, one process must be able to obtain two resources. This process
   requires no more resources and therefore it will return its resources when
   done.
   
* DONE Lab 4
  CLOSED: [2018-02-20 Tue 19:24] SCHEDULED: <2018-02-20 Tue> DEADLINE: <2018-02-21 Wed>
  #+BEGIN_SRC cpp :results output :tangle /ssh:kopsac@157.201.194.204#215:/home/kop14002/courses/cs345/lab4.cpp
    /***********************************************************************
     ,* Program:
     ,*    Lab Threads
     ,*    Brother Jones, CS 345
     ,* Author:
     ,*    Colton Kopsa
     ,* Summary:
     ,*    Perform M x N matrix multiplication on two matrices using threads.
     ,************************************************************************/
    #include <iostream>
    #include <pthread.h>
    #include <iomanip>
    using namespace std;

    #define M 5  // number of rows in matrix A
    #define K 6  // number of rows in matrix B -- number of columns in matrix A
    #define N 8  // number of columns in matrix B
    #define NUM_THREADS M*N

    // [rows][columns]
    int A [M][K] =
      {
        {1,4,3,7,9,1},
        {2,5,4,8,6,3},
        {3,6,5,8,2,3},
        {3,8,8,1,4,1},
        {1,5,4,5,7,9}
      }
      ;
    int B [K][N] =
      {
        {1,5,6,5,7,9,8,2},
        {1,2,3,5,5,6,7,8},
        {3,5,9,7,3,1,4,1},
        {8,3,1,2,6,5,2,4},
        {3,8,8,1,4, 1,3,3},
        {8,7,6,5,4,3,2,1}
      };
    int C [M][N];  // this is where the answer will be placed

    /* structure for passing data to threads */
    struct ThreadData
    {
      int i; /* row */
      int j; /* column */
    };

    void *print_message_function( void *ptr );

    int main()
    {
      pthread_t threads[NUM_THREADS];

      /* We have to create M * N worker threads */
      int threadCount = 0;
      for (int i = 0; i < M; i++)
        for (int j = 0; j < N; j++ )
          {
            ThreadData* tData = new ThreadData;
            tData->i = i;
            tData->j = j;
            /* Now create the thread passing it tData as
               a parameter */
            pthread_create( &threads[threadCount++], NULL, print_message_function, (void*) tData);
          }

      for (int i = 0; i < NUM_THREADS; i++)
        pthread_join(threads[i], NULL);

      cout << "The product of matrices A and B is:\n";
      for(int i = 0; i < M; i++)
        {
          for(int j = 0; j < N; j++)
            {
              cout << setw(8) << C[i][j];
            }
          cout << endl;
        }

      return 0;
    }

    void *print_message_function( void *ptr )
    {
      ThreadData *data;
      data = (ThreadData *) ptr;
      int i, sum = 0;
      for(i = 0; i < K; i++)
        {
          sum += A[data->i][i] * B[i][data->j];
        }
      C[data->i][data->j] = sum;
      pthread_exit(0);
    }
  #+END_SRC

  #+RESULTS:
  : The product of matrices A and B is:
  :      105     128     130      74     118      83      91      93
  :      125     133     137     100     135     107     107     101
  :      118     113     123     113     134     119     114     100
  :       63     113     153     122     111      95     128      95
  :      151     169     172     120     138     102     108      96
  
  #+BEGIN_SRC python :results output
    print "Howdy"
  #+END_SRC

  #+RESULTS:
  : Howdy
* DONE Weekly Reading [2/2]
  CLOSED: [2018-02-23 Fri 14:14]
** DONE Main memory (7.1 – 7.3)
   CLOSED: [2018-02-20 Tue 19:45] SCHEDULED: <2018-02-20 Tue> DEADLINE: <2018-02-21 Wed>
** DONE Main memory (7.4 – 7.5)
   CLOSED: [2018-02-23 Fri 14:14] SCHEDULED: <2018-02-23 Fri> DEADLINE: <2018-02-23 Fri>
* DONE HW #8 CS 345 Operating Systems Chapter 7 [9/9]
  CLOSED: [2018-02-28 Wed 15:07] SCHEDULED: <2018-02-27 Tue> DEADLINE: <2018-02-28 Wed>
** DONE (2) What is the difference between internal and external fragmentation?
   CLOSED: [2018-02-28 Wed 14:15]
   - Internal Fragmentation
     - When a process is allocated more memory than required, few space is left
       unused and this is called as INTERNAL FRAGMENTATION.
     - It occurs when memory is divided into fixed-sized partitions.
     - It can be cured by allocating memory dynamically or having partitions of
       different sizes.
   - External Fragmentation
     - After execution of processes when they are swapped out of memory and
       other smaller processes replace them, many small non contiguous(adjacent)
       blocks of unused spaces are formed which can serve a new request if all
       of them are put together but as they are not adjacent to each other a new
       request can't be served and this is known as EXTERNAL FRAGMENTATION.
     - It occurs when memory is divided into variable-sized partitions based on
       size of process.
     - It can be cured by Compaction, Paging and Segmentation.

** DONE (3) When might binding of logical addresses to physical addresses be done?
   CLOSED: [2018-02-28 Wed 14:23]
   - Before a program is executed.
*** What does ‘absolute code’ and ‘relocatable code’ have to do with the binding of addresses?
    If you know at compile time where the process will reside in memory, then
    absolute could can be generated. I think this means that virtual and
    physical address are the same. Binding is done at compile time.
    
    If it is not known at compile time where the process will reside in memory,
    then the compiler must generate relocatable code. The means that binding is
    done load time.
    
** DONE (2) What does MMU stand for and what does it do?
   CLOSED: [2018-02-28 Wed 14:24]
   Memory management unit, run-time mapping from virtual to physical addresses

** DONE (3) Discuss the pros and cons of worst fit as compared to first fit and best fit.
   CLOSED: [2018-02-28 Wed 14:28]
   - Best fit
     - Tends to leave very large holes and very small holes
     - Disadvantage: very small holes may be useless
   - First fit:
     - Tends to leave “average” size holes
     - Advantage: faster than best fit
   - Worst fit:
     - Simulation shows that worst fit is worst in terms of storage utilization

** DONE (2) What elements are typically found in a page table entry? Briefly define each element. (Refer to section 7.5.3 for this question.)
   CLOSED: [2018-02-28 Wed 14:34]

   The frame number, which tells the corresponding page in main memory.

   A modify (M) bit, which indicates whether the contents of the corresponding
   page have been altered since the page was last loaded into main memory.

   A present (P) bit, which indicates whether the corresponding page is in main
   memory or not.
   
   A reference bit may be included
** DONE (4.5) Given five memory partitions of 100 KB, 500 KB, 200 KB, 300 KB and 600 KB (in order), how would each of the first-fit, best-fit, and worst-fit algorithms place processes of 212 KB, 417 KB, 112 KB and 426 KB (in order)? Assume that the free space left when a block is allocated may be allocated for a future request. Which algorithm makes the most efficient use of memory?
   CLOSED: [2018-02-28 Wed 14:36]

   | First-fit | Best-fit | Worst-fit |
   |-----------+----------+-----------|
   | 100 KB    | 100 KB   | 100 KB    |
   | 500 KB    | 500 KB   | 500 KB    |
   | 200 KB    | 200 KB   | 200 KB    |
   | 300 KB    | 300 KB   | 300 KB    |
   | 600 KB    | 600 KB   | 600 KB    |
                         
   First-fit:
   - 212K is put in 500K partition
   - 417K is put in 600K partition
   - 112K is put in 288K partition (new partition 288K = 500K - 212K)
   - 426K must wait

   Best-fit:
   - 212K is put in 300K partition
   - 417K is put in 500K partition
   - 112K is put in 200K partition
   - 426K is put in 600K partition
   
   Worst-fit:
   - 212K is put in 600K partition
   - 417K is put in 500K partition
   - 112K is put in 388K partition
   - 426K must wait

   In this example, best-fit turns out to be the best.   

** DONE (2) What is the purpose of paging the page tables?
   CLOSED: [2018-02-28 Wed 14:38]
   In certain situations the page tables could become large enough that by paging
   the page tables, one could simplify the memory allocation problem (by
   ensuring that everything is allocated as fixed-size pages as opposed to
   variable-sized chunks) and also enable the swapping of portions of page table
   that are not currently used.
** DONE (4.5) Consider a system with memory mapping done on a page basis and using a single-level page table. Assume that the necessary page table is always in memory.
   CLOSED: [2018-02-28 Wed 14:40]
*** If a memory reference takes 200 ns, how long does it take to reference (or access) a memory location using the page table (a paged memory reference)?
    400 nanoseconds. 200 to get the page table entry, and 200 to access the memory location
*** Now we add an MMU that imposes an overhead (additional time) of 20 ns on each memory reference if it is a hit or a miss. If we assume that 85% of all memory references hit in the MMU TLB, what is the effective memory access time (EMAT)? [A memory reference to access the page table on a miss does not incur the time to access the TLB.]
    There are two cases. First, when the TLB contains the entry required, we
    have 20 ns overhead on top of the 200 ns memory access time. Second, when
    the TLB does not contain the item, we have an additional 200 ns to get the
    required entry into the TLB:
    - (220 * 0.85) + (420 * 0.15) = 250 ns
    - (100 * 0.65) + (200 * 0.35 + 5000000)
*** Generally speaking, what is the relationship between TLB hit rate and EMAT?
    The higher the TLB hit rate is, the smaller the EMAT is, because the
    additional 200 ns penalty to get the entry into the TLB contributes less to
    the EMAT.
** DONE (3) Suppose the page table for the process currently executing on the processor looks like the following. All numbers are decimal, everything is numbered starting from zero, and all addresses are memory byte addresses. The page size is 1024 bytes.
   CLOSED: [2018-02-28 Wed 14:46]

   |           Virtual page number | Valid bit | Page frame number              |
   |-------------------------------+-----------+--------------------------------|
   | (virtual address 0 - 1023)  0 |         1 | 4                              |
   |                             1 |         1 | 7                              |
   |                             2 |         0 | -                              |
   |                             3 |         1 | 2                              |
   |                             4 |         0 | -                              |
   |                             5 |         1 | (physical address 0 - 1023)  0 |

*** What physical address, if any would each of the following virtual addresses correspond to? (Do not try to handle page faults, if any, just indicate that there would be a page fault.)

**** 1052
     1 * 1024 + 28
     7 * 1024 + 28 => 7196

**** 2221
     2 * 1024 + 173
     ? * 1024 + 173 => Page Fault

**** 5499
     5 * 1024 + 379
     0 * 1024 + 379 => 379
* DONE Test 2
  CLOSED: [2018-02-23 Fri 11:26] SCHEDULED: <2018-02-22 Thu> DEADLINE: <2018-02-24 Sat>
* DONE Weekly Reading [3/3]
  CLOSED: [2018-03-02 Fri 14:50]
** DONE Main memory (7.6 – 7.9)
   CLOSED: [2018-02-26 Mon 14:48] SCHEDULED: <2018-02-26 Mon> DEADLINE: <2018-02-26 Mon>
** DONE Virtual memory (8.1 – 8.3)
   CLOSED: [2018-02-28 Wed 15:17] SCHEDULED: <2018-02-27 Tue> DEADLINE: <2018-02-28 Wed>
** DONE Virtual memory (8.4)
   CLOSED: [2018-03-02 Fri 14:50] SCHEDULED: <2018-03-01 Thu> DEADLINE: <2018-03-02 Fri>
* DONE Lab 5
  CLOSED: [2018-02-27 Tue 22:02] SCHEDULED: <2018-02-27 Tue> DEADLINE: <2018-02-27 Tue>
  [[file:producerConsumerLab.pdf][Lab 5]] 
  #+BEGIN_SRC C++ :tangle /ssh:kopsac@157.201.194.204#215:~/lab5.cpp :results raw
    /****************************************************************************
     ,* Program:
     ,*    Lab ProducerConsumerT2
     ,*    Brother Jones, CS 345
     ,* Author:
     ,*    Colton Kopsa
     ,*
     ,* Summary:
     ,*    This is an example of the producer consumer solution done with mutexs and
     ,*    semaphores.
     ,************************************************************************
     ,*
     ,* Changes made to my code for the re-submission:
     ,*   (examples:)
     ,*   - I fixed it to produce the correct number of consumer thread
     ,*   - I fixed the consume not notifying the semaphore when done
     ,*   - I added usage statement on incorrect arguments
     ,*
     ,*****************************************************************************/
    #include <semaphore.h>
    #include <pthread.h>
    #include <cstdlib>
    #include <unistd.h>
    #include <iostream>
    #include <iomanip>
    using namespace std;

    // Buffer Information
    #define BUFFER_SIZE 5
    typedef long bufferItem;
    bufferItem buffer[BUFFER_SIZE];

    // Thread Synchronization
    pthread_mutex_t myMutex;
    sem_t empty;
    sem_t full;
    int front = 0;

    /**
     ,* producer - generates a random value to insert into the shared buffer
     ,*/
    void *producer (void *param)
    {
      bufferItem itemProduced;
      while (true)
        {
          /* sleep for a random period of time */
          usleep(rand() % 150000);
          /* generate a random number */
          itemProduced = rand() % 1000;
          /* insert item into shared global buffer and print what was done */
          /* acquire the lock */
          sem_wait(&empty);
          pthread_mutex_lock(&myMutex);
          /*** critical section ***/
          int val;
          sem_getvalue(&full, &val);
          buffer[(val+front)%BUFFER_SIZE] = itemProduced;
          cout << setw(5)
               << itemProduced
               << setw(6)
               << "P"
               << (bufferItem) param << endl;
          /* release the lock */
          pthread_mutex_unlock(&myMutex);
          sem_post(&full);
        }
    }

    /**
     ,* consumer - consumes a random value to from the shared buffer
     ,*/
    void *consumer(void *param)
    {
      bufferItem consumedItem;
      while (true)
        {
          /* sleep for a random period of time */
          usleep(rand() % 150000);
          /* consume item from shared global buffer and print what was done */
          /* acquire the lock */
          sem_wait(&full);
          pthread_mutex_lock(&myMutex);
          /*** critical section ***/
          consumedItem = buffer[front];
          front = (front + 1) % BUFFER_SIZE;

          cout << setw(22)
               << consumedItem
               << setw(6)
               << "C"
               << (bufferItem) param+1 << endl;

          /* release the lock */
          pthread_mutex_unlock(&myMutex);
          sem_post(&empty);
        }
    }

    /**
     ,* main - simulates the consumer producer solution using semaphores and a mutex
     ,*/
    int main (int argc, char *argv[])
    {
      /*  1. Check and get command line arguments argv[1], argv[2], argv[3] */
      int sleepTime = 0; // defaults
      int producers = 0; // defaults
      int consumers = 0; // defaults
      if (argc == 4)
        {
          sleepTime = atoi(argv[1]);
          producers = atoi(argv[2]);
          consumers = atoi(argv[3]);
          cout << "Sleep time: " << sleepTime << endl;
          cout << "Number of Producers: " << producers << endl;
          cout << "Number of Consumers: " << consumers << endl;
        }
      else
        {
          cout << "Only " << argc << " argument(s) were provided." << endl;
          cout << "Correct usage: 'a.out sleepTime numProducers numConsumers'\n";

          return -1;
        }
      /*  2. Initialize buffer [good for error checking but not really needed]*/
      for(int i = 0; i < BUFFER_SIZE; i++)
        buffer[i] = 0;
      /*  3. Initialize the mutex lock and semaphores */

      /* create the mutex lock and initialize it */
      pthread_mutex_init(&myMutex, NULL);

      /* Create the semaphore and initialize it to 5 */
      sem_init(&empty, 0, BUFFER_SIZE);
      sem_init(&full, 0, 0);


      cout << "Produced  by P#  Consumed  by C#\n"
           << "========  =====  ========  =====\n";
      /*  4. Create producer threads(s) */
      pthread_t producerThreads[producers];
      for (long i = 0; i < producers; i++)
        {
          int error;
          error = pthread_create(&producerThreads[i], NULL, producer, (void*)i);
          if (error != 0)
            {
              cout << "Error creating producer thread: " << error << endl;
              return -1;
            }
        }
      /*  5. Create consumer threads(s) */
      pthread_t consumerThreads[consumers];
      for (long i = 0; i < consumers; i++)
        {
          int error;
          error = pthread_create(&consumerThreads[i], NULL, consumer, (void*)i);
          if (error != 0)
            {
              cout << "Error creating consumer thread: " << error << endl;
              return -1;
            }
        }
      /*  6. Sleep [ to read manual page, do:  man 3 sleep ]*/
      sleep(sleepTime);
      /*  7. Cancel threads [ pthread_cancel() ] */
      for (int i = 0; i < producers; i++)
        {
          pthread_cancel(producerThreads[i]);
        }
      for (int i = 0; i < consumers; i++)
        {
          pthread_cancel(consumerThreads[i]);
        }
      /*  8. Exit */
      return 0;
    }
  #+END_SRC
* DONE Lab 5 Review
  CLOSED: [2018-02-28 Wed 14:00] SCHEDULED: <2018-02-28 Wed> DEADLINE: <2018-03-01 Thu>
* DONE Lab 5 T-2 
  CLOSED: [2018-03-03 Sat 14:04] SCHEDULED: <2018-03-01 Thu> DEADLINE: <2018-03-03 Sat>
* DONE Lab 5 Grade Peers
  CLOSED: [2018-03-03 Sat 14:04] SCHEDULED: <2018-03-01 Thu> DEADLINE: <2018-03-03 Sat>
* DONE Weekly Reading [3/3]
  CLOSED: [2018-03-12 Mon 10:53]
** DONE Virtual memory (8.5, 8.6, 8.8, 8.9.1, 8.9.2, 8.9.5, 8.11)
   CLOSED: [2018-03-05 Mon 17:49] DEADLINE: <2018-03-05 Mon>
** DONE Parallel Programming : Sections 1-5 
   CLOSED: [2018-03-07 Wed 15:46] DEADLINE: <2018-03-07 Wed>
   https://computing.llnl.gov/tutorials/parallel_comp/
** DONE Parallel Programming : Sections 6-7.1
   CLOSED: [2018-03-12 Mon 10:58] DEADLINE: <2018-03-09 Fri>
   https://computing.llnl.gov/tutorials/parallel_comp/
* DONE Quiz #3 - Main Memory and Virtual Memory
  CLOSED: [2018-03-08 Thu 12:26] DEADLINE: <2018-03-08 Thu>
  https://byui.brightspace.com/d2l/lms/quizzing/quizzing.d2l?ou=377202&qi=762730
* DONE Lab #6 - Page Replacement Algorithms
  CLOSED: [2018-03-08 Thu 14:13] DEADLINE: <2018-03-09 Fri>
  https://byui.brightspace.com/d2l/le/content/377202/viewContent/5331213/View
* DONE HW #9 [6/7]
  CLOSED: [2018-03-07 Wed 15:46] DEADLINE: <2018-03-07 Wed>
  [[file:hw9-ch8_osc_wsosim.doc][Assignment Doc]] 
  
** DONE (3) What is the copy-on-write feature and under what circumstances is it beneficial to use this feature?
   CLOSED: [2018-03-07 Wed 12:15]
   - What is the hardware support required to implement this feature?

   Copy on Write allows processes to share pages rather than each having a
   separate copy of the pages. However, when one process tried to write to a
   shared page, then a trap is generated and the OS makes a separate copy of the
   page for each process. This is commonly used in a fork() operation where the
   child is supposed to have a complete copy of the parent address space. Rather
   than create a separate copy, the OS allows the parent and child to share the
   parent's pages. However, since each is supposed to have its own private copy
   of the pages, the pages are copied when one of them attemps a write. The
   hardware support required to implement is simply the following: on each
   memory access, the page table needs to be consulted to check whether the page
   is write-protected. If it is indeed write-protected, a trap would occur and
   the operating system could resolve the issue.
** DONE (4) What is thrashing and what causes it?
   CLOSED: [2018-03-05 Mon 14:37]
   In fact, look at any process that does not have “enough” frames. If the
   process does not have the number of frames it needs to support pages in
   active use, it will quickly page-fault. At this point, it must replace some
   page. However, since all its pages are in active use, it must replace a page
   that will be needed again right away. Consequently, it quickly faults again,
   and again, and again, replacing pages that it must bring back in immediately.
   This high paging activity is called /thrashing/.
*** How does the system detect thrashing?
    The system can detect thrashing by evaluating the level of CPU utilization
    as compared to the level of multiprogramming.
*** Once it detects thrashing, what can the system do to eliminate this problem?
    Decrease the degree of multiprogramming.
*** How can page-fault frequency be used to control thrashing?
    We can establish upper and lower bounds on the desired page-fault rate
    (Figure 8.21). If the actual page-fault rate exceeds the upper limit, we
    allocate the process another frame If the page-fault rate falls below the
    lower limit, we remove a frame from the process. Thus, we can directly
    measure and control the page-fault rate to prevent thrashing.
** DONE (2) What is the difference between a resident set and a working set?
   CLOSED: [2018-03-07 Wed 12:17]
    Resident set is that portion of the process image that is actually in
    real-memory at a particular instant. Working set is that subset of resident
    set that is actually needed for execution. (Relate this to the
    variable-window size method for swapping techniques.).
** DONE (2) What is the effect of setting ∆ to a small value on the page fault frequency and the number of active (non-suspended) processes currently executing in the system?
   CLOSED: [2018-03-07 Wed 15:09]
   Consider the parameter ∆ used to define the working-set window in the
   working-set model.
   - What is the effect when ∆ is set to a very high value?
     
   When Δ is set to a small value, then the set of resident pages for a process
   might be underestimated, allowing a process to be scheduled even though all
   of its required pages are not resident. This could result in a large number
   of page faults. When Δ is set to a large value, then a process’s resident set
   is overestimated and this might prevent many processes from being scheduled
   even though their required pages are resident. However, once a process is
   scheduled, it is unlikely to generate page faults since its resident set has
   been overestimated.
** DONE (3) Discuss situations under which the least frequently used page-replacement algorithm generates fewer page faults than the least recently used page-replacement algorithm.
   CLOSED: [2018-03-07 Wed 15:07]
   Consider the following sequence of memory accesses in a system that can hold
   four pages in memory: 1 1 2 3 4 5 1. When page 5 is accessed, the least
   frequently used page-replacement algorithm would replace a page other than 1,
   and therefore would not incur a page fault when page 1 is accessed again. On
   the other hand, for the sequence “1 2 3 4 5 2,” the least recently used
   algorithm performs better
** DONE (4) Consider a demand-paging system with the following time-measured utilizations:
   CLOSED: [2018-03-05 Mon 16:07]
   CPU utilization:  20%;		Paging disk:  97.7%		Other I/O devices:  5%

	 For each of the following, say whether it will (or is likely to) improve CPU
	 utilization. Briefly explain your answers:
  
   a.	Install a faster CPU
      - CPU utilization will go down because requests are getting sent faster.
   b.	Install a bigger (more GBs) paging disk.
      - Doesn't reduce page faults
   c.	Increase the degree of multiprogramming.
      - No, more applications will need more pages which increase thrashing.
   d.	Decrease the degree of multiprogramming.
      - Yes, if less applications are populating our pages, we will have less
        page faults.
   e.	Install more main memory.
      - Yes, more space in main memory will allow more pages to be stored
        reducing page faults.
   f. Install a faster hard disk or multiple controllers with multiple hard
   disks.
      - Could help by getting page faults done faster.
   g.	Increase the page size.
      - Page faults will happen despite page size.

** TODO Remainder of Assignment
#+BEGIN_SRC sh
  open hw9-ch8_osc_wsosim.doc
#+END_SRC

#+RESULTS:
* DONE Lab #6 Review
  CLOSED: [2018-03-12 Mon 16:08] DEADLINE: <2018-03-12 Mon>
* TODO Lab #6 - T2
  DEADLINE: <2018-03-14 Wed>
* DONE Lab 6 Grade Peers
  CLOSED: [2018-03-12 Mon 16:08] DEADLINE: <2018-03-12 Mon>
* TODO Weekly Reading [1/3]
** DONE Mass-Storage Structure (9.1 – 9.3)
   CLOSED: [2018-03-12 Mon 14:57] DEADLINE: <2018-03-12 Mon>
** TODO Mass-Storage Structure (9.4 – 9.6)
   DEADLINE: <2018-03-14 Wed>
** TODO Mass-Storage Structure (9.7 – 9.9)
   DEADLINE: <2018-03-16 Fri>
